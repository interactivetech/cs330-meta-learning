#### Maximum Likelihood Estimation
Maximum likelihood estimation is equivalent to minimizing forward KL divergence between data distribution and model distribution.
(Jensen-Shannon divergence is more similar to reverse KL.)

#### The taxonomy of MLE models
*  Explicit density models: 
    *  The density is tractable. 
    *  Otherwise usually need VI (deterministic) or MCMC (stochastic) methods for approximation. 
*  Implicit density models:
    *  For sampling, can use a Markov Chain to obtain a sequence of samples. 
    *  Or generate a sample within a single step. 

##### Fully Visible Belief Networks (WaveNet)
$$p(x) = \prod_{i=1}^n p(x_i\mid x_1, \dots, x_{i-1})$$.
* Tractable explicitl density model. 
* Inference cannot be parallized. 

##### Nonlinear independent components analysis (real NVP)
Given $$g$$ continuous, differentiable, invertible, by chain rule, $$p_x(x) = p_z(g^{-1}(x))\left|\det (\frac{\partial g^{-1}(x)}{\partial x})\right|$$. 
Then density $$p_x(x)$$ is tractable when $$p_z(\cdot)$$ and the Jacobian of the determinant of $$g^{-1}$$ is tractable. 
* The choice of $$g$$ is restricted. 
* Invertability of $$g$$ requires that $$z$$ has the same dimensionality as $$x$$. 

##### Variational Approximations (VAE)
$$\mathcal{L}(x; \theta) \leq \log p(x; \theta)$$.
* The equality may not be obtained after convergence, resulting in a model distribution different from the underlying data distribution.

##### Markov Chain Approximations (Boltzmann machines)
"A Markov chain is aprocess for generating samples by repeatedly drawing a sample $$x^\prime\sim q(x^\prime\mid x)$$. By repeatedly updating $$x$$ according to the transition operatorq, Markov chainmethods can sometimes guarantee that $$x$$ will eventually converge to a sample from $$p(x)$$."

##### Implicit density models (Generative stochastic networks, GAN)
MC-based approches are usually not scalable. 

#### Loss functions of GAN
"The relationship with variational autoencoders ismore complicated; the GAN framework can train some models that the VAE framework cannot and vice versa,  but the two frameworks  also have a largeintersection. The most salient difference is that, if relying on standard backprop, VAEs cannot have discrete variables at the input to the generator, while GANs cannot have discrete variables at the output of the generator."

##### Zero-Sum Game
The cost for the discriminator is $$J^{(D)} = -\frac{1}{2}\mathbb{E}_{p_\text{data}}\left[\log D(x)\right] - \frac{1}{2}\mathbb{E}_z\left[\log(1-D(G(z)))\right]$$.
Training the discriminator enables an estimate of the ratio $$\frac{p_\text{data}(x)}{p_\text{model}(x)}$$, which enables computation of some divergences and their gradients. "This  is  the  key  approximation  techniquethat sets GANs apart from variational autoencoders and Boltzmann machines.Other deep generative models make approximations based on lower bounds orMarkov chains; GANs make approximations based on using supervised learningto estimate a ratio of two densities."
>Because  the  GAN  framework  can  naturally  be  analyzed  with  the  tools  ofgame theory,  we call GANs “adversarial.”  But we can also think of them ascooperative, in the sense that the discriminator estimates this ratio of densitiesand then freely shares this information with the generator.  From this point ofview, the discriminator is more like a teacher instructing the generator in howto improve than an adversary.  So far, this cooperative view has not led to anyparticular change in the development of the mathematics.

In a zero-sum game, $$V = J^{(G)} = -J^{(D)}$$ is the discriminator's payoff. Then the game objective becomes $$\min_G\max_D V$$. "Learning in this game resembles minimizing the Jensen-Shannon divergence between the data and the model distribution, and the game converges to its equilibrium if both players’ policies can be updated directly in function space.  In practice, the players are represented with deep neural nets and updates are made in parameter space, so these results,  which depend on convexity, do not apply." 

##### Heuristic, Non-Saturating Game
To prevent vanishing gradient for the generator when the discriminator is confident, $$J^{(G)} = -\frac{1}{2} \mathbb{E}_z\left[\log D(G(z))\right]$$. Here rather than flipping the objective, the binary labels are flipped. 

##### MLE
To find a function $$f$$ such that $$J^{(G)} = \mathbb{E}_{p_g}\left[f(x)\right]$$ has the same gradient as $$\mathcal{D}_\text{KL}(p_\text{data}\|p_g)$$. 
Let $$G$$ be parameterized by $$\theta$$, then $$\frac{\partial}{\partial\theta}\mathcal{D}_\text{KL}(p_\text{data}\|p_g) =-\mathbb{E}_{p_\text{data}}\left[\frac{\partial}{\partial\theta}\log p_g(x)\right]$$, $$\frac{\partial}{\partial\theta}J^{(G)} =\mathbb{E}_{p_g}\left[f(x)\frac{\partial}{\partial\theta}\log p_g(x)\right]$$ (REINFORCE, under some assumptions).
When $$f(x) = \frac{p_\text{data}(x)}{p_g(x)}$$, the objective of the discriminator is equivalent to minimizing the forward KL divergence, in the sense that the gradients are the same. 
Now let $$D(x) = \sigma(a(x))$$ where $$\sigma(\cdot)$$ is the logistic sigmoid function. When the discriminator is optimal, $$D^*(x) = \frac{p_\text{data}(x)}{p_\text{data}(x) + p_g(x)}$$. 
Combining the above gives $$f(x) = -\exp(a(x))$$. That is, $$J^{(G)} = -\frac{1}{2}\mathbb{E}_z\left[\exp(\sigma^{-1}(D(G(z))))\right]$$, an MLE objective for the generator. 