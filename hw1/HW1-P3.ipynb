{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tf.set_random_seed(123)\n",
    "# np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mnist-hw1.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 3) (10000, 28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "data_trn, data_val = data['train'], data['test']\n",
    "print(data_trn.shape, data_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(\n",
    "    layer_in,\n",
    "    output_dim,\n",
    "    kernel_shape, # [kernel_height, kernel_width]\n",
    "    mask_type, # None, \"A\" or \"B\"\n",
    "    scope, \n",
    "    strides=[1, 1], # [column_wise_stride, row_wise_stride]\n",
    "    verbose=False):\n",
    "    with tf.variable_scope(scope, reuse=False):\n",
    "        mask_type = mask_type.lower()\n",
    "        batch_size, height, width, channel = layer_in.get_shape().as_list()\n",
    "        if verbose:\n",
    "            print(\"building within scope\", scope, batch_size, height, width, channel)\n",
    "        kernel_h, kernel_w = kernel_shape\n",
    "        stride_h, stride_w = strides\n",
    "\n",
    "        assert kernel_h % 2 == 1 and kernel_w % 2 == 1\n",
    "\n",
    "        center_h = kernel_h // 2\n",
    "        center_w = kernel_w // 2\n",
    "\n",
    "        weights = tf.get_variable(\"weights\", [kernel_h, kernel_w, channel, output_dim],\n",
    "                                  tf.float32, tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        if mask_type is not None:\n",
    "            mask = np.ones((kernel_h, kernel_w, channel, output_dim), dtype=np.float32)\n",
    "\n",
    "            mask[center_h, center_w+1:, :, :] = 0.\n",
    "            mask[center_h+1:, :, :, :] = 0.\n",
    "\n",
    "            if mask_type == 'a':\n",
    "                mask[center_h, center_w, :, :] = 0.\n",
    "\n",
    "            weights.assign(weights * tf.constant(mask, dtype=tf.float32))\n",
    "\n",
    "        layer_out = tf.nn.conv2d(input=layer_in, filter=weights, strides=[1, stride_h, stride_w, 1], \n",
    "                                 padding='SAME', name='layer_in_at_weights')\n",
    "        biases = tf.get_variable(\"biases\", [output_dim,], tf.float32, tf.zeros_initializer())\n",
    "        layer_out = tf.nn.bias_add(layer_out, biases, name='layer_in_at_weights_plus_biases')\n",
    "\n",
    "        layer_out = tf.nn.relu(layer_out, name='layer_out_activated')\n",
    "\n",
    "    return layer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(layer_in, hidden_dim, idx):\n",
    "    assert hidden_dim % 2 == 0\n",
    "    nn = conv2d(layer_in, output_dim=hidden_dim//2, kernel_shape=[1, 1], mask_type=\"B\", \n",
    "                scope=\"residual_block_0_\"+str(idx))\n",
    "    nn = conv2d(nn, output_dim=hidden_dim//2, kernel_shape=[3, 3], mask_type=\"B\", \n",
    "                scope=\"residual_block_1_\"+str(idx))\n",
    "    nn = conv2d(nn, output_dim=hidden_dim, kernel_shape=[1, 1], mask_type=\"B\", \n",
    "                scope=\"residual_block_2_\"+str(idx))\n",
    "    nn = tf.add(layer_in, nn, name=\"residual_block_out_\"+str(idx))\n",
    "    layer_out = tf.contrib.layers.layer_norm(nn, scope=\"residual_block_out_normalized_\"+str(idx))\n",
    "    return layer_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN():\n",
    "    def __init__(self, sess, color_dim=4, hidden_dim=16, out_hidden_dim=16, \n",
    "                 recurrent_length=12, out_recurrent_length=2, \n",
    "                 input_shape=[28, 28, 3], learning_rate=1e-3, grad_clip=1):\n",
    "\n",
    "        self.sess = sess\n",
    "        self.height, self.width, self.num_channels = input_shape\n",
    "        self.input = tf.placeholder(tf.float32, [None] + input_shape, name=\"input\")\n",
    "        \n",
    "        '''\n",
    "        build layers\n",
    "        '''\n",
    "        nn = conv2d(self.input, output_dim=hidden_dim, kernel_shape=[7, 7], mask_type=\"A\", \n",
    "                    scope=\"conv_in\")\n",
    "        for idx in range(recurrent_length):\n",
    "            nn = residual_block(nn, hidden_dim, idx)\n",
    "\n",
    "        for idx in range(out_recurrent_length):\n",
    "            nn = conv2d(nn, output_dim=out_hidden_dim, kernel_shape=[1, 1], mask_type=\"B\", \n",
    "                        scope=\"conv_out\"+str(idx))\n",
    "            nn = tf.nn.relu(nn, name='conv_out_activated_'+str(idx))\n",
    "            \n",
    "        self.logits = conv2d(nn, output_dim=self.num_channels*color_dim, kernel_shape=[1, 1], mask_type=\"B\", \n",
    "                             scope=\"conv_logits\")\n",
    "\n",
    "        '''\n",
    "        compute loss\n",
    "        \n",
    "        labels = self.input = [None, 28, 28, 3] -> [None, 28, 28, 3, 4] (one-hot encoded)\n",
    "        logits = self.logits = [None, 28, 28, 3*4] -> [None, 28, 28, 3, 4] (reshaped)\n",
    "        '''\n",
    "        self.logits = tf.reshape(self.logits, [-1, self.height, self.width, self.num_channels, color_dim], \n",
    "                                 name=\"logits_reshaped\")\n",
    "        self.probs = tf.nn.softmax(self.logits, axis=-1, name=\"probs\")\n",
    "        self.prediction = tf.argmax(self.probs, axis=-1, name=\"prediction\")\n",
    "        self.input_one_hot = tf.one_hot(tf.cast(self.input, tf.int32), color_dim, axis=-1, \n",
    "                                        name='input_one_hot')\n",
    "        self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                                   logits=self.logits, labels=self.input_one_hot, name='loss'))\n",
    "\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(self.loss)\n",
    "        new_grads_and_vars = \\\n",
    "            [(tf.clip_by_value(gv[0], -grad_clip, grad_clip), gv[1]) for gv in grads_and_vars]\n",
    "        self.op = optimizer.apply_gradients(new_grads_and_vars)\n",
    "        \n",
    "    def step(self, batch, with_update=False):\n",
    "        if with_update:\n",
    "            _, loss = self.sess.run([self.op, self.loss], feed_dict={self.input: batch})\n",
    "        else:\n",
    "            loss = self.sess.run(self.loss, feed_dict={self.input: batch})\n",
    "        return loss\n",
    "    \n",
    "    def generate(self, num_samples):\n",
    "        samples = np.zeros((num_samples, self.height, self.width, self.num_channels), dtype='float32')\n",
    "        for i in range(self.height):\n",
    "            for j in range(self.width):\n",
    "                for k in range(self.num_channels):\n",
    "                    samples[:, i, j, k] = self.sess.run(self.prediction, feed_dict={self.input: samples})[:, i, j, k]\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sess, data_trn, data_val, batch_size=512, num_epochs=5, log_per_epoch=1):\n",
    "    network = PixelCNN(sess)\n",
    "    init_op = tf.initializers.global_variables()\n",
    "    sess.run(init_op)\n",
    "\n",
    "    loss_trn = []\n",
    "    loss_val = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('starting epoch', epoch)\n",
    "        loss_trn_batch = []\n",
    "        for batch in np.array_split(data_trn, np.ceil(len(data_trn)/batch_size)):\n",
    "            loss = network.step(batch, with_update=True)\n",
    "            loss_trn_batch.append(loss)\n",
    "\n",
    "        if epoch % log_per_epoch == 0:\n",
    "            loss_trn.append(np.mean(loss_trn_batch))\n",
    "            loss_val.append(network.step(data_val, with_update=False))\n",
    "            print(loss_trn[-1], loss_val[-1])\n",
    "    return loss_trn, loss_val, network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small_data_trn = data_trn[:600]\n",
    "# small_data_val = data_val[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-59666dd00a2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mloss_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "loss_trn, loss_val, network = train(sess, data_trn, data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_trn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-1e1c21496e5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_trn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_trn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_trn' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = range(len(loss_trn))\n",
    "plt.plot(epochs, loss_trn)\n",
    "plt.plot(epochs, loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = network.generate(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(samples[0]/4, interpolation='nearest')\n",
    "plt.show()\n",
    "plt.imshow(samples[1]/4, interpolation='nearest')\n",
    "plt.show()\n",
    "plt.imshow(samples[2]/4, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/carpedm20/pixel-rnn-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
