{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('hw2_q2.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 32, 32, 3) (6838, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "data_trn, data_val = data['train'], data['test']\n",
    "print(data_trn.shape, data_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet(layer_in, scope, num_channels, output_dim, num_filters=256, num_blocks=8):\n",
    "    \n",
    "#     TODO: BatchNorm & WeightNormalization\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        h = conv2d(layer_in, scope=\"conv2d\", kernel=(3, 3), stride=(1, 1), \n",
    "                   in_channels=num_channels, out_channels=num_filters)\n",
    "        for idx in range(num_blocks):\n",
    "            _h = conv2d(h, scope=\"conv2d_\"+str(idx)+\"_0\", kernel=(1, 1), stride=(1, 1), \n",
    "                        in_channels=num_filters, out_channels=num_filters)\n",
    "            _h = tf.nn.relu(_h)\n",
    "            _h = conv2d(_h, scope=\"conv2d_\"+str(idx)+\"_1\", kernel=(3, 3), stride=(1, 1), \n",
    "                        in_channels=num_filters, out_channels=num_filters)\n",
    "            _h = tf.nn.relu(_h)\n",
    "            _h = conv2d(_h, kernel=(1,1), stride=(1, 1))\n",
    "            h = h + _h\n",
    "            h = tf.nn.relu(_h)\n",
    "        layer_out = conv2d(h, scope=\"resnet_layer_out\", kernel=(3, 3), stride=(1, 1), \n",
    "                           in_channels=num_filters, out_channels=output_dim)\n",
    "    return layer_out\n",
    "        \n",
    "def conv2d(layer_in, scope, kernel, stride, in_channels, out_channels):\n",
    "    with tf.variable_scope(scope):\n",
    "        kernel_h, kernel_w = kernel\n",
    "        stride_h, stride_w = stride\n",
    "        weights = tf.get_variable(\"weights\", [kernel_h, kernel_w, in_channels, out_channels],\n",
    "                                  tf.float32, tf.contrib.layers.xavier_initializer())\n",
    "        layer_out = tf.nn.conv2d(input=layer_in, filter=weights, strides=[1, stride_h, stride_w, 1], \n",
    "                                 padding='SAME', name='conv2d_layer_out')\n",
    "    return layer_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y=b \\odot x+(1-b) \\odot(x \\odot \\exp (s(b \\odot x))+t(b \\odot x))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def forward_and_jacobian(self, x, sum_log_det_jacobians, z):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def backward(self, y, z):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouplingLayer(Layer):\n",
    "    def __init__(self, scope, mask_type):\n",
    "        self.scope = scope\n",
    "        self.mask_type = mask_type\n",
    "        \n",
    "    def _get_mask(self, shape):\n",
    "        if self.mask_type.startswith(\"checkerboard\"):\n",
    "            if self.mask_type == \"checkerboard0\":\n",
    "                mask = tf.constant([[0.0, 1.0], [1.0, 0.0]], dtype=tf.float32)\n",
    "            elif self.mask_type == \"checkerboard1\": \n",
    "                mask = tf.constant([[1.0, 0.0], [0.0, 1.0]], dtype=tf.float32)\n",
    "            mask = tf.reshape(unit, [1, 2, 2, 1], name=\"mask_\"+mask_type)\n",
    "        elif self.mask_type.startswith(\"channel\"):\n",
    "            shape_halved = [shape[0], shape[1], shape[2], shape[3]//2]\n",
    "            ones = tf.ones(shape_halved)\n",
    "            zeros = tf.zeors(shape_halved)\n",
    "            if self.mask_type == \"channel0\":\n",
    "                mask = tf.concat([ones, zeros], axis=-1)\n",
    "            elif self.mask_type == \"channel1\": \n",
    "                mask = tf.concat([zeros, ones], axis=-1)\n",
    "        return mask\n",
    "    \n",
    "    def _build_log_s_t(self, masked_in, scope=\"_build_log_s_t\"):\n",
    "        with tf.variable_scope(scope):\n",
    "            resnet_out = resnet(masked_in, scope=\"resnet\", num_channels=self.C, output_dim=self.C*2)\n",
    "            log_s, t = split(resnet_out, 2, axis=-1)\n",
    "        return log_s, t\n",
    "    \n",
    "    def forward_and_jacobian(self, x, sum_log_det_jacobian, z):\n",
    "        '''\n",
    "        sum_log_det_jacobian = (None,)\n",
    "        '''\n",
    "        with tf.variable_scope(self.scope):\n",
    "            mask = self._get_mask(x.get_shape())\n",
    "            masked_x = mask * x\n",
    "            log_s, t = self._build_log_s_t(masked_x)\n",
    "            s = tf.check_numerics(tf.exp(log_s), \"exp has NaN\")\n",
    "            y = masked_x + (1 - mask) * (x * s + t)\n",
    "            sum_log_det_jacobian += tf.reduce_sum(log_s, axis=[1, 2, 3])\n",
    "            \n",
    "        return y, sum_log_det_jacobian, z \n",
    "    \n",
    "        # TODO: z changed?????????\n",
    "        \n",
    "    def backward(self, y, z):\n",
    "        with tf.variable_scope(self.scope, reuse=True):\n",
    "            mask = self._get_mask(y.get_shape())\n",
    "            masked_y = mask * y\n",
    "            log_s, t = self._build_log_s_t(mask_y)\n",
    "            neg_s = tf.check_numerics(tf.exp(-log_s), \"exp has NaN\")\n",
    "            x = masked_y + ((1 - mask) * y - t) * tf.exp(neg_s)\n",
    "            \n",
    "        return x, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezingLayer(Layer):\n",
    "    def __init__(self, scope):\n",
    "        self.name = name\n",
    "\n",
    "    def forward_and_jacobian(self, x, sum_log_det_jacobians, z):\n",
    "        y = tf.space_to_depth(x.get_shape(), 2)\n",
    "        if z is not None:\n",
    "            z = tf.space_to_depth(z, 2)      \n",
    "        return y,sum_log_det_jacobians, z\n",
    "    \n",
    "    def backward(self, y, z):\n",
    "        x = tf.depth_to_space(y.get_shape(), 2)\n",
    "        if z is not None:\n",
    "            z = tf.depth_to_space(z, 2)\n",
    "        return x, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.ops.array_ops.depth_to_space(input, block_size, name=None, data_format='NHWC')>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FactorOutLayer(Layer):\n",
    "    def __init__(self, scale, scope):\n",
    "        self.scale = scale\n",
    "        self.scope = scope\n",
    "    \n",
    "    def forward_and_jacobian(self, x, sum_log_det_jacobians, z):\n",
    "        split = x.get_shape()[-1]//2\n",
    "        \n",
    "        new_z = x[:, :, :, :split]\n",
    "        x = x[:, :, :, split:]\n",
    "        if z is not None:\n",
    "            z = tf.concat([z, new_z], axis=3)\n",
    "        else:\n",
    "            z = new_z\n",
    "        return x, sum_log_det_jacobians, z\n",
    "  \n",
    "    def backward(self, y, z):\n",
    "        # At scale 0, 1/2 of the original dimensions are factored out\n",
    "        # At scale 1, 1/4 of the original dimensions are factored out\n",
    "        # ....\n",
    "        # At scale s, (1/2)^(s+1) are factored out\n",
    "        # Hence, at backward pass of scale s, (1/2)^(s) of z should be factored in\n",
    "        \n",
    "        if y is not None:\n",
    "            split = y.get_shape()[-1]\n",
    "        else:\n",
    "            z.get_shape()[-1]//(2**self.scale)\n",
    "        \n",
    "        new_y = z[:, :, :, -split:]\n",
    "        z = z[:, :, :, :-split]\n",
    "        \n",
    "        if y is not None:\n",
    "            x = tf.concat([new_y, y], axis=3)\n",
    "        else:\n",
    "            x = new_y\n",
    "        \n",
    "        return x, z\n",
    "    \n",
    "    # Given the output of the network and all jacobians, \n",
    "    # compute the log probability. \n",
    "    # Equation (3) of the RealNVP paper\n",
    "    def compute_log_prob_x(z, sum_log_det_jacobians):\n",
    "\n",
    "        # y is assumed to be in standard normal distribution\n",
    "        # 1/sqrt(2*pi)*exp(-0.5*x^2)\n",
    "        zs = int_shape(z)\n",
    "        K = zs[1]*zs[2]*zs[3] #dimension of the Gaussian distribution\n",
    "\n",
    "        log_density_z = -0.5*tf.reduce_sum(tf.square(z), [1,2,3]) - 0.5*K*np.log(2*np.pi)\n",
    "\n",
    "        log_density_x = log_density_z + sum_log_det_jacobians\n",
    "\n",
    "        # to go from density to probability, one can \n",
    "        # multiply the density by the width of the \n",
    "        # discrete probability area, which is 1/256.0, per dimension.\n",
    "        # The calculation is performed in the log space.\n",
    "        log_prob_x = log_density_x - K*tf.log(256.0)\n",
    "\n",
    "        return log_prob_x\n",
    "\n",
    "\n",
    "    # Computes the loss of the network. \n",
    "    # It is chosen so that the probability P(x) of the \n",
    "    # natural images is maximized.\n",
    "    def loss(z, sum_log_det_jacobians):\n",
    "        return -tf.reduce_mean(compute_log_prob_x(z, sum_log_det_jacobians))\n",
    "    \n",
    "\n",
    "    # Adam optimizer.\n",
    "    # Exactly the same code as the PixelCNN++ implementation by OpenAI.\n",
    "    # https://github.com/openai/pixel-cnn\n",
    "    def adam_updates(params, cost_or_grads, lr=0.001, mom1=0.9, mom2=0.999):\n",
    "        updates = []\n",
    "        if type(cost_or_grads) is not list:\n",
    "            grads = tf.gradients(cost_or_grads, params)\n",
    "        else:\n",
    "            grads = cost_or_grads\n",
    "        t = tf.Variable(1., 'adam_t')\n",
    "        for p, g in zip(params, grads):\n",
    "            mg = tf.Variable(tf.zeros(p.get_shape()), p.name + '_adam_mg')\n",
    "            if mom1 > 0: \n",
    "                v = tf.Variable(tf.zeros(p.get_shape()), p.name + '_adam_v')\n",
    "                v_t = mom1*v + (1. - mom1)*g\n",
    "                v_hat = v_t / (1. - tf.pow(mom1,t))\n",
    "                updates.append(v.assign(v_t))\n",
    "            else:\n",
    "                v_hat = g\n",
    "            mg_t = mom2*mg + (1. - mom2)*tf.square(g)\n",
    "            mg_hat = mg_t / (1. - tf.pow(mom2,t))\n",
    "            g_t = v_hat / tf.sqrt(mg_hat + 1e-8)\n",
    "            p_t = p - lr * g_t\n",
    "            updates.append(mg.assign(mg_t))\n",
    "            updates.append(p.assign(p_t))\n",
    "        updates.append(t.assign_add(1))\n",
    "    return tf.group(*updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
