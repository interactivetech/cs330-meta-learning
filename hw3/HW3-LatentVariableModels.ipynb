{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data_1():\n",
    "    count = 100000\n",
    "    rand = np.random.RandomState(0)\n",
    "    return [[1.0, 2.0]] + rand.randn(count, 2) * [[5.0, 1.0]]\n",
    "def sample_data_2():\n",
    "    count = 100000\n",
    "    rand = np.random.RandomState(0)\n",
    "    return [[1.0, 2.0]] + (rand.randn(count, 2) * [[5.0, 1.0]]).dot(\n",
    "    [[np.sqrt(2) / 2, np.sqrt(2) / 2], [-np.sqrt(2) / 2, np.sqrt(2) / 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = sample_data_1()\n",
    "data_1_trn, data_1_val = data_1[:80000], data_1[80000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(input_ph, output_dim, scope, num_layers, hidden_dim, activation=tf.tanh, output_activation=None):\n",
    "    output_ph = input_ph\n",
    "    with tf.variable_scope(scope):\n",
    "        for _ in range(num_layers):\n",
    "            output_ph = tf.layers.dense(output_ph, hidden_dim, activation=activation)\n",
    "        output_ph = tf.layers.dense(output_ph, output_dim, activation=output_activation)\n",
    "    return output_ph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$l_{i}(\\theta, \\phi)=\\mathbb{E}_{z \\sim q_{\\theta}\\left(z | x_{i}\\right)}\\left[-\\log p_{\\phi}\\left(x_{i} | z\\right)\\right]+K L\\left(q_{\\theta}\\left(z | x_{i}\\right) \\| p(z)\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE():\n",
    "    def __init__(self, sess, x_size=2, z_size=2, learning_rate=1e-4):\n",
    "        self.sess = sess\n",
    "        self.x = tf.placeholder(tf.float32, (None, x_size), name=\"x_ph\")\n",
    "        self.prior = self._make_prior(z_size)\n",
    "        self.z = self.prior.sample(tf.shape(self.x)[0])\n",
    "        self.encoder = self._make_encoder(self.x, z_size)\n",
    "        self.decoder = self._make_decoder(self.z, x_size)\n",
    "        \n",
    "        self.loss = self._build_loss\n",
    "        self.op = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "        \n",
    "        self.num_samples = tf.placeholder(tf.int32, (), name=\"num_samples\")\n",
    "        self.z_sp = self.prior.sample(self.num_samples)\n",
    "        self.decoder_sp = self._make_decoder(self.z_sp, x_size)\n",
    "        self.x_sp = self.decoder_sp.sample(self.num_samples)\n",
    "        \n",
    "    def _build_nn(self, layer_in, output_dim, scope, num_layers=2, hidden_dim=6):\n",
    "        stats = build_mlp(layer_in, 2 * output_dim, scope, num_layers, hidden_dim)\n",
    "        mean, std = tf.split(stats, 2, axis=-1)\n",
    "        dist = tfp.distributions.MultivariateNormalDiag(loc=mean, scale_diag=std, name=scope)\n",
    "        return dist\n",
    "    \n",
    "    def _make_prior(self, z_size):\n",
    "        mean = tf.Variable(tf.zeros((z_size), tf.float32), name=\"prior_mean\")\n",
    "        std = tf.Variable(tf.zeros((z_size), tf.float32), name=\"prior_logstd\")\n",
    "        return tfp.distributions.MultivariateNormalDiag(loc=mean, scale_diag=std, name='prior')\n",
    "    \n",
    "    def _make_encoder(self, x, z_size):\n",
    "        make_encoder = tf.make_template('encoder', self._build_nn)\n",
    "        encoder = make_encoder(x, z_size, 'z')\n",
    "        return encoder\n",
    "    \n",
    "    def _make_decoder(self, z, x_size):\n",
    "        make_decoder = tf.make_template('decoder', self._build_nn)\n",
    "        decoder = make_decoder(z, x_size, 'x')\n",
    "        return decoder\n",
    "        \n",
    "    def _build_loss(self):\n",
    "        entropy = - self.decoder.log_prob(self.x, name=\"decoder_log_prob\")\n",
    "        kl = self.encoder.kl_divergence(self.prior)\n",
    "        loss = tf.reduce_mean(entropy, axis=0) + kl\n",
    "        return loss\n",
    "        \n",
    "    def step(self, batch, with_update=False):\n",
    "        if with_update:\n",
    "            loss, _ = self.sess.run([self.loss, self.op], feed_dict={self.x: batch})\n",
    "        else:\n",
    "            loss = self.sess.run(self.loss, feed_dict={self.x: batch})\n",
    "        return loss\n",
    "    \n",
    "    def sample(self, num_samples):\n",
    "        samples = self.sess.run(self.x_sp, feed_dict={self.num_samples: num_samples})\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sess, data_trn, data_val, batch_size=64, num_epochs=60, \n",
    "          log_per_epoch=1, print_per_epoch=1):\n",
    "    print(\"building model...\")\n",
    "    model = VAE(sess)\n",
    "    init_op = tf.initializers.global_variables()\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    loss_trn = []\n",
    "    loss_val = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"epoch {} starts...\".format(epoch))\n",
    "        loss_trn_batch = []\n",
    "        for batch in np.array_split(data_trn, np.ceil(len(data_trn)/batch_size)):\n",
    "            loss = model.step(batch, with_update=True)\n",
    "            loss_trn_batch.append(loss)\n",
    "\n",
    "        if epoch % log_per_epoch == 0:\n",
    "            loss_trn.append(np.mean(loss_trn_batch))\n",
    "            loss_val.append(model.step(data_val, with_update=False))\n",
    "\n",
    "        if epoch % print_per_epoch == 0:\n",
    "            print(\"at epoch\", epoch, loss_trn[-1], loss_val[-1])\n",
    "            \n",
    "    return loss_trn, loss_val, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trn, data_val = data_1_trn, data_1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [] and loss <bound method VAE._build_loss of <__main__.VAE object at 0x1c2a8ccac8>>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-39bfc0b4a97a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mloss_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-52-0850e7164595>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(sess, data_trn, data_val, batch_size, num_epochs, log_per_epoch, print_per_epoch)\u001b[0m\n\u001b[1;32m      2\u001b[0m           log_per_epoch=1, print_per_epoch=1):\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"building model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0minit_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-1084fb679fb5>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sess, x_size, z_size, learning_rate)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"num_samples\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    408\u001b[0m           \u001b[0;34m\"No gradients provided for any variable, check your graph for ops\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m           \u001b[0;34m\" that do not support gradients, between variables %s and loss %s.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m           ([str(v) for _, v in grads_and_vars], loss))\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [] and loss <bound method VAE._build_loss of <__main__.VAE object at 0x1c2a8ccac8>>."
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "loss_trn, loss_val, model = train(sess, data_trn, data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = model.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
